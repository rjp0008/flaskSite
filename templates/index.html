<!DOCTYPE html>
<html lang="en">

<head>

    <title>Robert Perez</title>

    <link href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" rel="stylesheet">
    <link href="/static/css/perez.css" rel="stylesheet">


</head>
<body>
    <div class="container">

        <div class="row">

            <div class="col-lg-8">
                <h1>Robert Perez</h1>


                <hr>
                <img class="img-responsive" src="/static/me.JPG" alt="">
                      <hr>
                 <p class="lead">Exploring OpenCV</p>
                <p>July 22, 2017 at 3:00 PM</p>
                <p> Recently, I have spent some time playing around with OpenCV in python testing out some image recognition capabilities of the library and consuming various hardware streams.
                    I thought that creating a home security-ish solution could be a fun project. After some initial investigations with OpenCV following tutorials like the one <a href="http://www.pyimagesearch.com/2015/11/09/pedestrian-detection-opencv/">here</a>
                     I decided to pursue it further. My dad had mentioned in the past that he wanted to set up a security camera in his shed to monitor his tools, as he's been burgled before. So I figured this was something I could attempt for the experience and
                figured I would save him a bit in subscription costs as compared to the cloud solutions he had seen. An old android phone or web cam probably wouldn't cut it for this project, though, as night vision would be an issue along with image quality. I settled on the Vimtag VT-361
                 wifi enabled security, I figured I could consume the stream as easily as IP camera or my web cam, but unfortunately ended up bring wrong.</p>
                <p>Getting data from the camera was the first major initial roadblock. The web interface is a flash player that I couldn't quite figure how to capture the stream and the alternative video feed was an installed ActiveX control, neither of these were great options. Luckily there is a
                snapshot button on the web interface that brought up a window of the latest screen cap. Inspecting this element led me to the url I can hit that exposes the picture via a jpg. Now I'm in business, I was able to run this jpg through the initial people detecting algorithm that
                played around with earlier, unfortunately it detected some objects in the picture as people too. I needed a better solution. </p>
                <p>Template matching is the answer I am currently settled upon. After exploring the docs for OpenCV there was a <a href="http://docs.opencv.org/trunk/d4/dc6/tutorial_py_template_matching.html">handy example</a> for pattern matching and recognizing templates in a photo.
                    I printed out a large QR code to tape to the back of the door that would serve as the template and could fed the latest snapshots into this algorithm to detect the missing or moved QR code. This was a lot more reliable that the human detection. After setting it to capture images infrequently
                to capture data points to make sure it works at all times of day I realized that all my photos after a certain period were the same. I manually visited the URL page again and it was the same image as the others I have saved. After logging in again to see the snapshot and inspeccting the element
                I realized what happened. There is an expiring token passed in the url, I suppose as an added security measure, and as far as I can tell it's just a randomly generated string. So I had to automate grabbing the latest image url, unfortunately the website was giving me a lot of trouble with
                google chrome selinium and I didn't want to have a browser pop up randomly anyways. PhantomJS came to the rescue, it's a super lightweight selenium compatible browser with an invisible mode compatible with this proprietary security camera website.</p>
                <p>The next step will be to have a watchdog process and make sure the analysis is always online, sending alerts as sms and then possibly a native app.</p>

                      <hr>
                 <p class="lead">Consuming a Google Sheets page as a data store</p>
                <p> May 21, 2017 at 4:00 PM</p>
                <p>Once again it's been a couple weeks since updating. So I wanted to get back into the swing of things here by working on just a simple locally hosted website.
                 It's going to be a simple website for my parents to use since I pay their bills online and they reimburse me. I wanted to give them a html page they could have
                 on their machine that would just load the latest bills I've paid without them. Sure, they could just use the google sheets sharable page but where's the challenge in that?</p>
                <p>Unfortunately, this page won't be in any of my repo's for now and will continue not to be unless I end up extending it to be more useful and scrub the private information out.</p>
                <p>I knew I had heard of using Google Sheets as a backend database before for simple projects so I did a couple of searches. The first thing I found was blockspring.com however I'm
                not really interested in a subscription fee so that was a non-starter. The <a href="https://coderwall.com/p/duapqq/use-a-google-spreadsheet-as-your-json-backend">
                second result </a>I found seemed fine for my current use case. All this required was publishing my Sheet with a read only link, and pasting the document key into a url request.
                 Visiting this url showed the json in my browser and I checked to make sure it was valid json.</p>
                <p>We have the <i>"database"</i> now.</p>
                <p>From here, I began with a barebones .html page and imported bootstrap and jQuery from CDNs. I added a table that will host the information with columns for company, date paid, date due,
                    and amount. I created a .js file to handle the ajax requests and populate said table. The ajax request happens on load and performs a GET request to the Google Sheets URL
                I have already established. Parsing this response as a JSON object allowed me to navigate down to the entry object, which are the rows of
                the sheet. I could see if the reimbursed tag was marked and ignore entries that have already been paid. From there it was a insertRow call on the table and then insertCell calls on the row to get the data into the page.
                I added an accumulator on the amounts being parsed so I can have a total at the bottom of the amounts column. As soon as I put out this data I got <a href="http://www.smbc-comics.com/?id=2999">3062.1400000000003</a>.
                    Adding a decimal format to this value got a valid amount of money and now I have my simple site with a Google Sheets backend!</p>


      <hr>
                 <p class="lead">Refining a tool for /r/churning </p>
                <p> April 30, 2017 at 7:00 PM</p>
                <p>It's been a while since I posted anything here, I think I got off to a good start but slacked on updates
                throughout the month of April. My current goal is to have at least three updates or posts on here a month, but a bare minimum of one. I'll make the goal this time, just barely.
                </p>
                <p>I have a <a href="https://github.com/rjp0008/referralChecker">repository</a> on github of a referral checker for <a href="http://www.reddit.com/r/churning">/r/churning</a> that I hoped to use to catch when the wrong offers are posted
                to referral threads.</p>
                <p>I've done some basic work here with the python library PRAW, but it was really just some initial testing and sandboxing. Coming back to this code made me realize how important well commented and refactored code can be.
                Going through and debugging the test script is a great reminder of the track I was on 2-3 months ago when I last worked on this. The first thing it does is go to the referrals wiki page and get all the links for the individual referral threads.
                Then it searches for the platinum delta referral thread, because that was the starting point I wanted to use, as many links here were the 35k bonus when a 70k bonus was available. All the comments in the thread are traversed, which are all just links to
                American Express application pages. Then the text of the application page is put into a dictionary counter that adds one for each instance of the application page landing text. After all this data is scraped the text is compared with difflib library to get the offers.</p>
                <p>In my research, I found that the rankt sire does some of this already. For example, the Delta platinum referral page has everything linked grouped into point bonuses. Most people have linked the 50k application but there were two links for the
                plain American Express platinum card. I reached out to these users so they could correct their post and to the creator of rankt top see if anything was being developed to fix this. I could extend my code into checking for this now. But I'm going to wait a bit for a response
                in case he is already implementing a fix.</p>
                <p>With that I'm going to cut today's post short, but I think this week I'll either extend my code to check and make sure the product linked in referral threads is the correct one or get back to the <a href="https://github.com/rjp0008/hoteldata">hotel tracking platform</a>.</p>
                <hr>
                <p class="lead">Gathering data from WeatherUnderground and plotting with Plotly </p>
                <p> Posted on March 17, 2017 at 7:00 PM</p>
                <p>So recently I set up a Rasberry Pi in my home office to log data from a DHT11 sensor. This is a budget
                sensor that can get temperature and humidity data at 1 hz. That is plenty enough data for me, unfortunately
                the sensor only has a resolution of 1 degree celsius and 1% humidity. I'm waiting on an updated sensor to get
                here from aliexpress and I'll do a more detailed writeup about the specific set-up when that comes in.</p>
                <p>For now, I have days worth of temperature data from my office, the result of which can probably be guessed.
                If you want to look at the data, you can look at the recent logs I've gathered <a href="/temperatureData">here</a>
                or a graphical representation <a href="/tempMap3/hours=24">here</a>.</p>
                <p>For this blog post I'll be extending the graphical representation of the above data to display information
                logged from my hometown.</p>
                <p>The first step in this process is getting more data, and for that, I used the free service at<a href="https://www.wunderground.com/weather/api">https://www.wunderground.com</a>.
                I compared this API to a similar one at openweathermap.org but decided the 10 calls per minute limit for wunderground was just fine for what I'm doing. My
                initial goal is to just get weather data for myself after all, and querying the weather at home even once a minute is probably overkill. As it shouldn't change that often.</p>
                <p>After I made my account for the API it's time to start querying it, for that I opened up the Postman chrome browser extension. A pretty great tool for exploring and testing RESTful APIs.
                    I was able to copy the <a href="http://api.wunderground.com/api/2a340340b2e9fae1/conditions/q/CA/San_Francisco.json">example url</a> from the documentation and swap in my API key, and city state combo.
                    It immediately came up with plenty of data in the response, which I may save and explore in a future update, but for now I verified that the temperture and humidity were the correct numbers.
                </p>
                <p>Now that I've found the correct URL endpoint to hit, it's time to add the calls to my python code in my rasberry pi. I've made http requests in python before, but Postman will generate the
                    code required to make this same request in the future. I output the python 3 code in postman, which is using the http.client package to visit the url and grab the data. </p>
                <script src="https://gist.github.com/rjp0008/1716f0acd7b4b3b3cfb9d21b35a7fd4b.js"></script>
                <p>After decoding the response from this http request into a json object I was able to index it for the ['temp_c'] and ['relative_humidity'] data. I noticed that this API also gives you data in Fahrenheit,
                which will end up being more precise as it looks like both C and F datapoints are only to the tenths decimal place. However all the current data points are in C so I'll have to convert the value to match the old data.
                The conversion factor is tC = (tF - 32) * 5 / 9, which I almost remembered, but had to double check online to be sure. So after I had this data I piped it into another http request to this site, which logs the data passed
                into a SQLite database. The only data fields I keep track of right now are temperture, humidity, and location(inside or outside).</p>
                <p>Data is being logged from two points 24/7, and I wanted to graph these things against each other. Now I already have the display endpoint mentioned earlier for just the inside data, but now I need to add the new data points
                into the graphs. My initial display engine utilizing pandas package dataframe and plot left a lot to be desired. With a little researching I found a much slicker presentation out of the box with Plotly. The <a href="https://github.com/plotly/plotlyjs-flask-example">
                        example code</a> provided by plotly for use in flask was the exact thing I needed. I'm definitely impressed by the ease of use and quality of plotly and will visit it again in another post.</p>
                <p>The last thing I did for this was to set up the script to run on boot, in case of power loss on the pi it would continue logging points asap. In windows it's a lot easier to just add things to the startup folder and they just run on boot. From what I could find,
                doing a similar thing for linux is a little more involved. I ended up just adding the path to my script to the rc.local file in /etc directory. Unfortunately I had to use vi, and that's always a struggle, I had the shortcut commands open on my second monitor.
                After adding the entry to run my script in that file, I rebooted my pi and it started logging data automatically without me having to ssh in and kick off the script!</p>
<hr>
                <p class="lead">Cleaning up this site.</p>
                <p> March 6, 2017 at 7:00 PM</p>
                <p>So this site is hosted on a flask server, specifically one of the many running
                    at <a href="https://www.pythonanywhere.com">pythonanywhere.com</a>.
                    It's been a great (and cheap) resource, but by tooling around and just doing things for fun has resulted in my folder
                    and file organization becoming a mess. All of my routes are in one main flask app and the three projects
                    I have going are all over the place in that file.
                </p>
                <p>Another issue with my current development path is that not much of it is in source control. I was just tooling
                around with some basic stuff so I didn't think to bother with it. At this point though I think it's reached a
                point where I have to start organizing and using git like I know I should.</p>

                <p>Anyways my first task that gave me trouble is using css in this flask served webpage. I tried to create
                    a css file inside the static/css directory for my site. Specifically I was creating a codeblock class to use
                in posts. However flask was not happy with linking both bootstrap and my css file as shown below.</p>
       <script src="https://gist.github.com/rjp0008/cd3c463b6a26d0fecf04f408b2230fca.js"></script>
                <p>After reading some blog posts, and realizing this isn't an effort in web design, I decided
                to use githubs inline gists if I ever needed to inline code. I know I don't have an eye for visual design,
                so using out of the box solutions that look great is the best solution for me.</p>
                <p>The first refactoring effort I'm doing is to bring the intergraph endpoint into it's own file.
                I created this endpoint as one of the first slack custom addins I made for my team. It's purpose is
                pretty simple, it will return a random person on the team in a json formatted response. This
                ties into slacks webhoob functionality to give use a /random slash command in any slack channel. Mostly this
                is used to make someone finally getting around to doing a code review. There are some bonuses to the slash
                command though, it will remove the requester from the pool of output options because you can't do your own
                code review. Also you can @tag or :emojii: tag people on the team to get them removed from the pool,
                if they've already done one of the reviews.</p>
                <p>In refactoring this slash command I've realized that now there is no way to arbitrarily choose any person on
                on the team. This is because the requester will always be excluded from the pool of options. The solution
                I came up with is to have two endpoints, a truely random person endpoint, and a code review endpoint. They are a
                'GET' and 'POST' http request on the perezdev.com/random/ url respectively. This was possible by marking the
                    specified route with an extra parameter 'methods=['GET', 'POST']'. The default for flask is to olny support
                    GET requests. I decided this distinction between GET/POST because you can either get any random person, or post data to the url that excludes team members.</p>
                <hr>

                <p class="lead">Starting a development blog for my adventures in coding and scripting.</p>
                <p> March 5, 2017 at 3:00 PM</p>
                <p>I finally decided to start a personal site after reading other people's and using them as very helpful resources.
                Hopefully something I post here will be as useful as the things I've read in the past.</p>
                <p>I expect many of my initial blog posts will be summations of the work I've already done as part of different sites hosted here.
                Most of this aforementioned work is only half completed or could be extended. Which is part of the reason I started this
                site, to create accountability and finish up some of these projects.</p>
                <hr>
            </div>
        </div>
        <footer>
            <div class="row">
                <div class="col-lg-12">
                    <p>Copyright &copy; PerezDev 2017</p>
                </div>
            </div>
        </footer>

    </div>
    <script src="{{ url_for('static',filename='js/jquery.js') }}"></script>
    <script src="{{ url_for('static',filename='js/bootstrap.min.js') }}"></script>

</body>

</html>
